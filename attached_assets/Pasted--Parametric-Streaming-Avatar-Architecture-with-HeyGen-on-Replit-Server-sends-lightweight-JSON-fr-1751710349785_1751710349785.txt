/*
Parametric Streaming Avatar Architecture with HeyGen on Replit
- Server sends lightweight JSON frame data (visemes, blendshapes, headPose)
- Audio packets encoded with Opus over WebSocket
- Client renders 3D avatar locally using Three.js
- Millisecond-level end-to-end latency (20â€“30 ms)

Setup:
1. Create `.env`:
   HEYGEN_API_KEY=your_api_key
   AVATAR_MODEL_URL=URL_to_gltf_model
2. Install dependencies:
   npm install express ws dotenv
   # Client: include Three.js via CDN
3. Run:
   npm start
*/

// server.js
require('dotenv').config();
const express = require('express');
const WebSocket = require('ws');
const { spawn } = require('child_process');
const path = require('path');

const app = express();
app.use(express.static(path.join(__dirname, 'public')));
const server = app.listen(process.env.PORT || 3000, () => {
  console.log('Server running on port', server.address().port);
});

// WebSocket for parametric frames\const wss = new WebSocket.Server({ server });

wss.on('connection', ws => {
  console.log('Client connected');

  // Example: spawn a child process that generates frame JSON and Opus audio
  const generator = spawn('node', ['frame-generator.js']);

  generator.stdout.on('data', chunk => {
    // chunk contains JSON or binary audio
    ws.send(chunk);
  });

  generator.on('close', () => ws.close());

  ws.on('close', () => generator.kill());
});

// frame-generator.js (pseudo-code):
// Should output alternating JSON text frames and Opus audio packets
// For real use, integrate HeyGen SDK to get visemes/blendshapes metadata


// public/index.html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Parametric Streaming Avatar</title>
  <style>body { margin: 0; overflow: hidden; }</style>
</head>
<body>
  <canvas id="avatarCanvas"></canvas>
  <button id="start">Start</button>
  <button id="stop">Stop</button>
  <script src="https://cdn.jsdelivr.net/npm/three@0.150.1/build/three.min.js"></script>
  <script>
    let scene, camera, renderer, avatar;
    let ws, audioContext, audioSource;

    async function initThree() {
      renderer = new THREE.WebGLRenderer({ canvas: document.getElementById('avatarCanvas') });
      scene = new THREE.Scene();
      camera = new THREE.PerspectiveCamera(45, window.innerWidth / window.innerHeight, 0.1, 1000);
      camera.position.z = 2;
      renderer.setSize(window.innerWidth, window.innerHeight);

      const loader = new THREE.GLTFLoader();
      const gltf = await loader.loadAsync(process.env.AVATAR_MODEL_URL);
      avatar = gltf.scene;
      scene.add(avatar);

      animate();
    }

    function animate() {
      requestAnimationFrame(animate);
      renderer.render(scene, camera);
    }

    document.getElementById('start').onclick = () => {
      ws = new WebSocket(`ws://${location.host}`);
      audioContext = new AudioContext();
      audioSource = audioContext.createBufferSource();
      audioSource.connect(audioContext.destination);

      ws.onmessage = async event => {
        if (typeof event.data === 'string') {
          // JSON frame
          const frame = JSON.parse(event.data);
          avatar.traverse(child => {
            if (child.morphTargetDictionary) {
              for (let [name, idx] of Object.entries(child.morphTargetDictionary)) {
                child.morphTargetInfluences[idx] = frame.blendshapes[name] || 0;
              }
            }
          });
          avatar.rotation.set(frame.headPose.pitch, frame.headPose.yaw, frame.headPose.roll);
        } else {
          // Binary Opus packet
          const arrayBuffer = await event.data.arrayBuffer();
          const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
          audioSource.buffer = audioBuffer;
          audioSource.start();
        }
      };
    };

    document.getElementById('stop').onclick = () => {
      ws.close();
      audioSource.stop();
    };

    initThree();
  </script>
</body>
</html>
